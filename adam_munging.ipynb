{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 03:06:30,197] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/.conda/envs/jax311/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/adam/.conda/envs/jax311/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/adam/.conda/envs/jax311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba\n",
      "For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3\n",
      "For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer\n"
     ]
    }
   ],
   "source": [
    "from megatron.neox_arguments import NeoXArgs\n",
    "from megatron.training import setup_model_and_optimizer\n",
    "from megatron.initialize import initialize_megatron\n",
    "from megatron import mpu\n",
    "\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeoXArgs.from_ymls() ['/mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/global_step256/configs/pythia-14m.yml']\n",
      "\n",
      "Set args\n",
      "\n",
      "\n",
      "Setting up model parallel world size...\n",
      "\n",
      "\n",
      "Set model parallel world size\n",
      "\n",
      "\n",
      "Setting up model parallel rank...\n",
      "\n",
      "\n",
      "Set model parallel rank\n",
      "\n",
      "\n",
      "Initializing distributed...\n",
      "\n",
      "[2025-01-22 03:06:33,455] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-01-22 03:06:33,456] [INFO] [comm.py:667:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 03:06:33,803] [INFO] [comm.py:718:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.146.101.79, master_port=29500\n",
      "[2025-01-22 03:06:33,804] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "\n",
      "Setting up model parallel...\n",
      "\n",
      "> initializing model parallel with size 1\n",
      "\n",
      "Initializing Megatron...\n",
      "\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "_initialize_distributed() model parallel is already initialized\n",
      "[2025-01-22 03:06:33,809] [INFO] [checkpointing.py:1125:configure] Activation Checkpointing Information\n",
      "[2025-01-22 03:06:33,809] [INFO] [checkpointing.py:1126:configure] ----Partition Activations False, CPU CHECKPOINTING False\n",
      "[2025-01-22 03:06:33,810] [INFO] [checkpointing.py:1127:configure] ----contiguous Memory Checkpointing False with 6 total layers\n",
      "[2025-01-22 03:06:33,810] [INFO] [checkpointing.py:1128:configure] ----Synchronization True\n",
      "[2025-01-22 03:06:33,811] [INFO] [checkpointing.py:1129:configure] ----Profiling time in checkpointing False\n",
      "> setting random seeds to 1234 ...\n",
      "[2025-01-22 03:06:33,813] [INFO] [checkpointing.py:229:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "\n",
      "Setting up model and optimizer...\n",
      "\n",
      "building GPT2 model ...\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(pipe=0, data=0): 0}\n",
      "[2025-01-22 03:06:33,815] [INFO] [module.py:398:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp\n",
      "stage=0 layers=11\n",
      "     0: EmbeddingPipe\n",
      "     1: _pre_transformer_block\n",
      "     2: ParallelTransformerLayerPipe\n",
      "     3: ParallelTransformerLayerPipe\n",
      "     4: ParallelTransformerLayerPipe\n",
      "     5: ParallelTransformerLayerPipe\n",
      "     6: ParallelTransformerLayerPipe\n",
      "     7: ParallelTransformerLayerPipe\n",
      "     8: _post_transformer_block\n",
      "     9: NormPipe\n",
      "    10: ParallelLinearPipe\n",
      "  loss: partial\n",
      "num_embeddings=50304\n",
      "num_embeddings=50304\n",
      "Configuring Optimizer type: Adam with params: {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}\n",
      "WARNING: APEX not installed - defaulting to deepspeed's fused adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/adam/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/adam/.cache/torch_extensions/py311_cu124/fused_adam/build.ninja...\n",
      "/home/adam/.conda/envs/jax311/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.0392146110534668 seconds\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2025-01-22 03:06:34,023] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.3, git-hash=unknown, git-branch=unknown\n",
      "[2025-01-22 03:06:34,024] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 03:06:34,212] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-01-22 03:06:34,214] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-01-22 03:06:34,214] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-01-22 03:06:34,215] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-01-22 03:06:34,216] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2025-01-22 03:06:34,229] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer\n",
      "[2025-01-22 03:06:34,230] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-01-22 03:06:34,231] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f6df40c3d10>\n",
      "[2025-01-22 03:06:34,232] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]\n",
      "[2025-01-22 03:06:34,233] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-01-22 03:06:34,233] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-01-22 03:06:34,234] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-01-22 03:06:34,235] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-01-22 03:06:34,235] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-01-22 03:06:34,236] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-01-22 03:06:34,236] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-01-22 03:06:34,237] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-01-22 03:06:34,237] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-01-22 03:06:34,238] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-01-22 03:06:34,238] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-01-22 03:06:34,239] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6edd488c10>\n",
      "[2025-01-22 03:06:34,239] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-01-22 03:06:34,240] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-01-22 03:06:34,241] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-01-22 03:06:34,241] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-01-22 03:06:34,241] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-01-22 03:06:34,242] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-01-22 03:06:34,242] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-01-22 03:06:34,243] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-01-22 03:06:34,243] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-01-22 03:06:34,244] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2025-01-22 03:06:34,244] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-01-22 03:06:34,244] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-01-22 03:06:34,245] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-01-22 03:06:34,245] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-01-22 03:06:34,246] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-01-22 03:06:34,246] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-01-22 03:06:34,247] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-01-22 03:06:34,247] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-01-22 03:06:34,248] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-01-22 03:06:34,248] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-01-22 03:06:34,248] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2025-01-22 03:06:34,249] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2025-01-22 03:06:34,249] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-01-22 03:06:34,250] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-01-22 03:06:34,250] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-01-22 03:06:34,251] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-01-22 03:06:34,251] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-01-22 03:06:34,252] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-01-22 03:06:34,252] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-01-22 03:06:34,253] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-01-22 03:06:34,253] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 4096\n",
      "[2025-01-22 03:06:34,253] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-01-22 03:06:34,254] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-01-22 03:06:34,254] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-01-22 03:06:34,255] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-01-22 03:06:34,255] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-01-22 03:06:34,256] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-01-22 03:06:34,256] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-01-22 03:06:34,256] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-01-22 03:06:34,257] [INFO] [config.py:1003:print]   optimizer_name ............... adam\n",
      "[2025-01-22 03:06:34,258] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}\n",
      "[2025-01-22 03:06:34,258] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-01-22 03:06:34,259] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-01-22 03:06:34,259] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-01-22 03:06:34,260] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-01-22 03:06:34,260] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-01-22 03:06:34,260] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-01-22 03:06:34,261] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-01-22 03:06:34,261] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-01-22 03:06:34,262] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-01-22 03:06:34,262] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-01-22 03:06:34,263] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-01-22 03:06:34,263] [INFO] [config.py:1003:print]   train_batch_size ............. 1\n",
      "[2025-01-22 03:06:34,264] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-01-22 03:06:34,264] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-01-22 03:06:34,264] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-01-22 03:06:34,265] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... True\n",
      "[2025-01-22 03:06:34,265] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-01-22 03:06:34,266] [INFO] [config.py:1003:print]   world_size ................... 1\n",
      "[2025-01-22 03:06:34,266] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-01-22 03:06:34,267] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-01-22 03:06:34,267] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2025-01-22 03:06:34,267] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-01-22 03:06:34,268] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2025-01-22 03:06:34,269] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.9, 0.95], \n",
      "            \"eps\": 1e-08\n",
      "        }\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"fp16\": true, \n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true\n",
      "}\n",
      " > number of parameters on model parallel rank 0: 14067712\n",
      " > total params: 14,067,712\n",
      "[2025-01-22 03:06:35,097] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/global_step256/mp_rank_00_model_states.pt...\n",
      "[2025-01-22 03:06:35,169] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/global_step256/mp_rank_00_model_states.pt.\n",
      "[2025-01-22 03:06:35,177] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/global_step256/mp_rank_00_model_states.pt...\n",
      "[2025-01-22 03:06:35,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/global_step256/mp_rank_00_model_states.pt.\n",
      "module_state_dict.keys()=odict_keys(['sequential.0.word_embeddings.weight', 'sequential.2.input_layernorm.weight', 'sequential.2.input_layernorm.bias', 'sequential.2.attention.query_key_value.weight', 'sequential.2.attention.query_key_value.bias', 'sequential.2.attention.rotary_emb.inv_freq', 'sequential.2.attention.dense.weight', 'sequential.2.attention.dense.bias', 'sequential.2.post_attention_layernorm.weight', 'sequential.2.post_attention_layernorm.bias', 'sequential.2.mlp.dense_h_to_4h.weight', 'sequential.2.mlp.dense_h_to_4h.bias', 'sequential.2.mlp.dense_4h_to_h.weight', 'sequential.2.mlp.dense_4h_to_h.bias', 'sequential.3.input_layernorm.weight', 'sequential.3.input_layernorm.bias', 'sequential.3.attention.query_key_value.weight', 'sequential.3.attention.query_key_value.bias', 'sequential.3.attention.rotary_emb.inv_freq', 'sequential.3.attention.dense.weight', 'sequential.3.attention.dense.bias', 'sequential.3.post_attention_layernorm.weight', 'sequential.3.post_attention_layernorm.bias', 'sequential.3.mlp.dense_h_to_4h.weight', 'sequential.3.mlp.dense_h_to_4h.bias', 'sequential.3.mlp.dense_4h_to_h.weight', 'sequential.3.mlp.dense_4h_to_h.bias', 'sequential.4.input_layernorm.weight', 'sequential.4.input_layernorm.bias', 'sequential.4.attention.query_key_value.weight', 'sequential.4.attention.query_key_value.bias', 'sequential.4.attention.rotary_emb.inv_freq', 'sequential.4.attention.dense.weight', 'sequential.4.attention.dense.bias', 'sequential.4.post_attention_layernorm.weight', 'sequential.4.post_attention_layernorm.bias', 'sequential.4.mlp.dense_h_to_4h.weight', 'sequential.4.mlp.dense_h_to_4h.bias', 'sequential.4.mlp.dense_4h_to_h.weight', 'sequential.4.mlp.dense_4h_to_h.bias', 'sequential.5.input_layernorm.weight', 'sequential.5.input_layernorm.bias', 'sequential.5.attention.query_key_value.weight', 'sequential.5.attention.query_key_value.bias', 'sequential.5.attention.rotary_emb.inv_freq', 'sequential.5.attention.dense.weight', 'sequential.5.attention.dense.bias', 'sequential.5.post_attention_layernorm.weight', 'sequential.5.post_attention_layernorm.bias', 'sequential.5.mlp.dense_h_to_4h.weight', 'sequential.5.mlp.dense_h_to_4h.bias', 'sequential.5.mlp.dense_4h_to_h.weight', 'sequential.5.mlp.dense_4h_to_h.bias', 'sequential.6.input_layernorm.weight', 'sequential.6.input_layernorm.bias', 'sequential.6.attention.query_key_value.weight', 'sequential.6.attention.query_key_value.bias', 'sequential.6.attention.rotary_emb.inv_freq', 'sequential.6.attention.dense.weight', 'sequential.6.attention.dense.bias', 'sequential.6.post_attention_layernorm.weight', 'sequential.6.post_attention_layernorm.bias', 'sequential.6.mlp.dense_h_to_4h.weight', 'sequential.6.mlp.dense_h_to_4h.bias', 'sequential.6.mlp.dense_4h_to_h.weight', 'sequential.6.mlp.dense_4h_to_h.bias', 'sequential.7.input_layernorm.weight', 'sequential.7.input_layernorm.bias', 'sequential.7.attention.query_key_value.weight', 'sequential.7.attention.query_key_value.bias', 'sequential.7.attention.rotary_emb.inv_freq', 'sequential.7.attention.dense.weight', 'sequential.7.attention.dense.bias', 'sequential.7.post_attention_layernorm.weight', 'sequential.7.post_attention_layernorm.bias', 'sequential.7.mlp.dense_h_to_4h.weight', 'sequential.7.mlp.dense_h_to_4h.bias', 'sequential.7.mlp.dense_4h_to_h.weight', 'sequential.7.mlp.dense_4h_to_h.bias', 'sequential.9.norm.weight', 'sequential.9.norm.bias', 'sequential.10.final_linear.weight'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SequentialWrapper:\n\tMissing key(s) in state_dict: \"sequential.2.mlp.linear1.weight\", \"sequential.2.mlp.linear1.bias\", \"sequential.2.mlp.linear2.weight\", \"sequential.2.mlp.linear2.bias\", \"sequential.3.mlp.linear1.weight\", \"sequential.3.mlp.linear1.bias\", \"sequential.3.mlp.linear2.weight\", \"sequential.3.mlp.linear2.bias\", \"sequential.4.mlp.linear1.weight\", \"sequential.4.mlp.linear1.bias\", \"sequential.4.mlp.linear2.weight\", \"sequential.4.mlp.linear2.bias\", \"sequential.5.mlp.linear1.weight\", \"sequential.5.mlp.linear1.bias\", \"sequential.5.mlp.linear2.weight\", \"sequential.5.mlp.linear2.bias\", \"sequential.6.mlp.linear1.weight\", \"sequential.6.mlp.linear1.bias\", \"sequential.6.mlp.linear2.weight\", \"sequential.6.mlp.linear2.bias\", \"sequential.7.mlp.linear1.weight\", \"sequential.7.mlp.linear1.bias\", \"sequential.7.mlp.linear2.weight\", \"sequential.7.mlp.linear2.bias\". \n\tUnexpected key(s) in state_dict: \"sequential.2.attention.rotary_emb.inv_freq\", \"sequential.2.mlp.dense_h_to_4h.weight\", \"sequential.2.mlp.dense_h_to_4h.bias\", \"sequential.2.mlp.dense_4h_to_h.weight\", \"sequential.2.mlp.dense_4h_to_h.bias\", \"sequential.3.attention.rotary_emb.inv_freq\", \"sequential.3.mlp.dense_h_to_4h.weight\", \"sequential.3.mlp.dense_h_to_4h.bias\", \"sequential.3.mlp.dense_4h_to_h.weight\", \"sequential.3.mlp.dense_4h_to_h.bias\", \"sequential.4.attention.rotary_emb.inv_freq\", \"sequential.4.mlp.dense_h_to_4h.weight\", \"sequential.4.mlp.dense_h_to_4h.bias\", \"sequential.4.mlp.dense_4h_to_h.weight\", \"sequential.4.mlp.dense_4h_to_h.bias\", \"sequential.5.attention.rotary_emb.inv_freq\", \"sequential.5.mlp.dense_h_to_4h.weight\", \"sequential.5.mlp.dense_h_to_4h.bias\", \"sequential.5.mlp.dense_4h_to_h.weight\", \"sequential.5.mlp.dense_4h_to_h.bias\", \"sequential.6.attention.rotary_emb.inv_freq\", \"sequential.6.mlp.dense_h_to_4h.weight\", \"sequential.6.mlp.dense_h_to_4h.bias\", \"sequential.6.mlp.dense_4h_to_h.weight\", \"sequential.6.mlp.dense_4h_to_h.bias\", \"sequential.7.attention.rotary_emb.inv_freq\", \"sequential.7.mlp.dense_h_to_4h.weight\", \"sequential.7.mlp.dense_h_to_4h.bias\", \"sequential.7.mlp.dense_4h_to_h.weight\", \"sequential.7.mlp.dense_4h_to_h.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Now load model and optimizer\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSetting up model and optimizer...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m model, optimizer, lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43msetup_model_and_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/gpt-neox/megatron/training.py:1328\u001b[0m, in \u001b[0;36msetup_model_and_optimizer\u001b[0;34m(neox_args, use_cache, iteration)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be using deepspeed to run neox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neox_args\u001b[38;5;241m.\u001b[39mload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1328\u001b[0m     neox_args\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneox_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneox_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_reference_model:\n\u001b[1;32m   1336\u001b[0m         _ \u001b[38;5;241m=\u001b[39m load_checkpoint(\n\u001b[1;32m   1337\u001b[0m             neox_args\u001b[38;5;241m=\u001b[39mneox_args,\n\u001b[1;32m   1338\u001b[0m             model\u001b[38;5;241m=\u001b[39mreference_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1341\u001b[0m             iteration\u001b[38;5;241m=\u001b[39miteration,\n\u001b[1;32m   1342\u001b[0m         )\n",
      "File \u001b[0;32m~/src/gpt-neox/megatron/checkpointing.py:390\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(neox_args, model, optimizer, lr_scheduler, inference, iteration)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m checkpoint_name, state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneox_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_optimizer_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_optim_and_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_lr_scheduler_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_optim_and_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_module_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_optim_and_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_module_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneox_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_impl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# if an iteration is specified, we want to raise an error here rather than\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# continuing silently, since we are trying to load a specific checkpoint\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/deepspeed/runtime/engine.py:2852\u001b[0m, in \u001b[0;36mDeepSpeedEngine.load_checkpoint\u001b[0;34m(self, load_dir, tag, load_module_strict, load_optimizer_states, load_lr_scheduler_states, load_module_only, custom_load_fn)\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_has_ckpt_event_prologue():\n\u001b[1;32m   2849\u001b[0m     \u001b[38;5;66;03m# Prepare for checkpoint load by ensuring all parameters are partitioned\u001b[39;00m\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mcheckpoint_event_prologue()\n\u001b[0;32m-> 2852\u001b[0m load_path, client_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mload_module_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_module_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mload_optimizer_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_optimizer_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mload_lr_scheduler_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_lr_scheduler_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mload_module_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_module_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mcustom_load_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_load_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2860\u001b[0m load_zero_checkpoint \u001b[38;5;241m=\u001b[39m load_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_optimization() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbfloat16_enabled())\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_zero_checkpoint:\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/deepspeed/runtime/engine.py:2935\u001b[0m, in \u001b[0;36mDeepSpeedEngine._load_checkpoint\u001b[0;34m(self, load_dir, tag, load_module_strict, load_optimizer_states, load_lr_scheduler_states, load_module_only, custom_load_fn)\u001b[0m\n\u001b[1;32m   2926\u001b[0m     DeepSpeedEngine\u001b[38;5;241m.\u001b[39mload_moe_state_dict(load_dir,\n\u001b[1;32m   2927\u001b[0m                                         tag,\n\u001b[1;32m   2928\u001b[0m                                         state_dict\u001b[38;5;241m=\u001b[39mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                                         num_experts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_experts,\n\u001b[1;32m   2933\u001b[0m                                         checkpoint_engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_engine)\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_universal_checkpoint():\n\u001b[0;32m-> 2935\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2936\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_module_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2937\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcustom_load_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_load_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2938\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfetch_z3_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_z3_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_checkpoint_dp_world_size \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdp_world_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   2942\u001b[0m optim_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/deepspeed/runtime/engine.py:2712\u001b[0m, in \u001b[0;36mDeepSpeedEngine.load_module_state_dict\u001b[0;34m(self, checkpoint, strict, custom_load_fn, fetch_z3_params)\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2711\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_state_dict\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2712\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO\u001b[39;49;00m\n\u001b[1;32m   2714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mget(FROZEN_PARAM_FRAGMENTS, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2717\u001b[0m     saved_frozen_params \u001b[38;5;241m=\u001b[39m checkpoint[FROZEN_PARAM_FRAGMENTS]\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SequentialWrapper:\n\tMissing key(s) in state_dict: \"sequential.2.mlp.linear1.weight\", \"sequential.2.mlp.linear1.bias\", \"sequential.2.mlp.linear2.weight\", \"sequential.2.mlp.linear2.bias\", \"sequential.3.mlp.linear1.weight\", \"sequential.3.mlp.linear1.bias\", \"sequential.3.mlp.linear2.weight\", \"sequential.3.mlp.linear2.bias\", \"sequential.4.mlp.linear1.weight\", \"sequential.4.mlp.linear1.bias\", \"sequential.4.mlp.linear2.weight\", \"sequential.4.mlp.linear2.bias\", \"sequential.5.mlp.linear1.weight\", \"sequential.5.mlp.linear1.bias\", \"sequential.5.mlp.linear2.weight\", \"sequential.5.mlp.linear2.bias\", \"sequential.6.mlp.linear1.weight\", \"sequential.6.mlp.linear1.bias\", \"sequential.6.mlp.linear2.weight\", \"sequential.6.mlp.linear2.bias\", \"sequential.7.mlp.linear1.weight\", \"sequential.7.mlp.linear1.bias\", \"sequential.7.mlp.linear2.weight\", \"sequential.7.mlp.linear2.bias\". \n\tUnexpected key(s) in state_dict: \"sequential.2.attention.rotary_emb.inv_freq\", \"sequential.2.mlp.dense_h_to_4h.weight\", \"sequential.2.mlp.dense_h_to_4h.bias\", \"sequential.2.mlp.dense_4h_to_h.weight\", \"sequential.2.mlp.dense_4h_to_h.bias\", \"sequential.3.attention.rotary_emb.inv_freq\", \"sequential.3.mlp.dense_h_to_4h.weight\", \"sequential.3.mlp.dense_h_to_4h.bias\", \"sequential.3.mlp.dense_4h_to_h.weight\", \"sequential.3.mlp.dense_4h_to_h.bias\", \"sequential.4.attention.rotary_emb.inv_freq\", \"sequential.4.mlp.dense_h_to_4h.weight\", \"sequential.4.mlp.dense_h_to_4h.bias\", \"sequential.4.mlp.dense_4h_to_h.weight\", \"sequential.4.mlp.dense_4h_to_h.bias\", \"sequential.5.attention.rotary_emb.inv_freq\", \"sequential.5.mlp.dense_h_to_4h.weight\", \"sequential.5.mlp.dense_h_to_4h.bias\", \"sequential.5.mlp.dense_4h_to_h.weight\", \"sequential.5.mlp.dense_4h_to_h.bias\", \"sequential.6.attention.rotary_emb.inv_freq\", \"sequential.6.mlp.dense_h_to_4h.weight\", \"sequential.6.mlp.dense_h_to_4h.bias\", \"sequential.6.mlp.dense_4h_to_h.weight\", \"sequential.6.mlp.dense_4h_to_h.bias\", \"sequential.7.attention.rotary_emb.inv_freq\", \"sequential.7.mlp.dense_h_to_4h.weight\", \"sequential.7.mlp.dense_h_to_4h.bias\", \"sequential.7.mlp.dense_4h_to_h.weight\", \"sequential.7.mlp.dense_4h_to_h.bias\". "
     ]
    }
   ],
   "source": [
    "\n",
    "YAML_PATH = \"/mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/global_step256/configs/pythia-14m.yml\"\n",
    "CHECKPOINT_PATH = \"/mnt/hdd-0/tiny-pythia/ckpts/pythia-14m/\" # mp_rank_00_model_states.pt\n",
    "\n",
    "# 1. Load the configuration\n",
    "args = NeoXArgs.from_ymls([YAML_PATH])\n",
    "\n",
    "# Set the load path for the checkpoint and other necessary args\n",
    "args.load = CHECKPOINT_PATH\n",
    "args.vocab_size = 50304\n",
    "args.padded_vocab_size = 50304\n",
    "\n",
    "# Add fusion settings\n",
    "args.scaled_upper_triang_masked_softmax_fusion = False\n",
    "args.bias_gelu_fusion = False\n",
    "args.masked_softmax_fusion = False\n",
    "\n",
    "# Add MP settings\n",
    "args.rank = 0\n",
    "args.world_size = 1\n",
    "args.model_parallel_size = 1\n",
    "args.pipe_parallel_size = 1\n",
    "args.num_stages = 1\n",
    "args.num_layers_per_virtual_pipeline_stage = 1\n",
    "\n",
    "# Add DeepSpeed settings\n",
    "args.train_batch_size = 1\n",
    "args.train_micro_batch_size_per_gpu = 1\n",
    "args.zero_optimization = {\"stage\": 0}\n",
    "\n",
    "print(\"\\nSet args\\n\")\n",
    "\n",
    "print(\"\\nSetting up model parallel world size...\\n\")\n",
    "mpu.set_model_parallel_world_size(1)  # Since you're using model parallel size of 1\n",
    "print(\"\\nSet model parallel world size\\n\")\n",
    "\n",
    "print(\"\\nSetting up model parallel rank...\\n\")\n",
    "mpu.set_model_parallel_rank(0)  # Since you have rank 0\n",
    "print(\"\\nSet model parallel rank\\n\")\n",
    "\n",
    "# Initialize distributed backend first\n",
    "print(\"\\nInitializing distributed...\\n\")\n",
    "deepspeed.init_distributed()\n",
    "\n",
    "# Then set up model parallel groups\n",
    "print(\"\\nSetting up model parallel...\\n\")\n",
    "mpu.initialize_model_parallel(model_parallel_size=1)\n",
    "\n",
    "# Initialize Megatron after MP setup\n",
    "print(\"\\nInitializing Megatron...\\n\")\n",
    "initialize_megatron(args)\n",
    "\n",
    "# Now load model and optimizer\n",
    "print(\"\\nSetting up model and optimizer...\\n\")\n",
    "model, optimizer, lr_scheduler = setup_model_and_optimizer(args, iteration=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 02:20:21,798] [INFO] [comm.py:656:init_distributed] cdb=None\n",
      "[2025-01-22 02:20:21,799] [INFO] [comm.py:671:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 02:20:22,168] [INFO] [comm.py:722:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.146.101.79, master_port=29500\n",
      "[2025-01-22 02:20:22,169] [INFO] [comm.py:687:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "> initializing model parallel with size 1\n"
     ]
    }
   ],
   "source": [
    "# import deepspeed\n",
    "# import megatron.mpu as mpu\n",
    "\n",
    "# # Initialize process group for DeepSpeed\n",
    "# deepspeed.init_distributed(\n",
    "#     # dist_backend='nccl',\n",
    "#     # #distributed_port=12355,\n",
    "#     # verbose=True\n",
    "# )\n",
    "\n",
    "# # Initialize MPU variables\n",
    "# mpu.initialize_model_parallel(1)  # Set model parallel size to 1 for single GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
